<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="(1) 神经网络神经网络的表示Ⅰ神经网络起源神经网络是在模拟大脑中的神经元时发明的。神经元是大脑中的细胞，神经元有一个细胞主体(Cell body)，有一定数量的输入神经，这些输入神经叫做树突(Dendrite)。可以把它们想象成输入电线，它们接收来自其他神经元的信息。神经元的输出神经叫做轴突(Axon)，这些输出神经是用来给其他神经元传递信号或者传送信息的。 简而言之，神经元是一个计算单元，它从">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="斯坦福机器学习课程 第四周">
<meta property="og:url" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/index.html">
<meta property="og:site_name" content="|喵|">
<meta property="og:description" content="(1) 神经网络神经网络的表示Ⅰ神经网络起源神经网络是在模拟大脑中的神经元时发明的。神经元是大脑中的细胞，神经元有一个细胞主体(Cell body)，有一定数量的输入神经，这些输入神经叫做树突(Dendrite)。可以把它们想象成输入电线，它们接收来自其他神经元的信息。神经元的输出神经叫做轴突(Axon)，这些输出神经是用来给其他神经元传递信号或者传送信息的。 简而言之，神经元是一个计算单元，它从">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/001.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/002.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/003.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/004.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/005.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/006.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/007.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/008.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/009.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/010.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/011.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/012.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/013.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/014.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/015.jpg">
<meta property="og:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/016.jpg">
<meta property="og:updated_time" content="2017-08-17T12:41:36.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="斯坦福机器学习课程 第四周">
<meta name="twitter:description" content="(1) 神经网络神经网络的表示Ⅰ神经网络起源神经网络是在模拟大脑中的神经元时发明的。神经元是大脑中的细胞，神经元有一个细胞主体(Cell body)，有一定数量的输入神经，这些输入神经叫做树突(Dendrite)。可以把它们想象成输入电线，它们接收来自其他神经元的信息。神经元的输出神经叫做轴突(Axon)，这些输出神经是用来给其他神经元传递信号或者传送信息的。 简而言之，神经元是一个计算单元，它从">
<meta name="twitter:image" content="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/001.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/"/>





  <title>斯坦福机器学习课程 第四周 | |喵|</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">|喵|</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">喵喵喵</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/17/Stanford-MachineLearning-week4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="El Chiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="|喵|">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">斯坦福机器学习课程 第四周</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-17T17:49:29+08:00">
                2017-08-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-神经网络"><a href="#1-神经网络" class="headerlink" title="(1) 神经网络"></a>(1) 神经网络</h2><h3 id="神经网络的表示Ⅰ"><a href="#神经网络的表示Ⅰ" class="headerlink" title="神经网络的表示Ⅰ"></a>神经网络的表示Ⅰ</h3><h4 id="神经网络起源"><a href="#神经网络起源" class="headerlink" title="神经网络起源"></a>神经网络起源</h4><p>神经网络是在模拟大脑中的神经元时发明的。神经元是大脑中的细胞，神经元有一个细胞主体(Cell body)，有一定数量的输入神经，这些输入神经叫做<strong>树突(Dendrite)</strong>。可以把它们想象成输入电线，它们接收来自其他神经元的信息。神经元的输出神经叫做<strong>轴突(Axon)</strong>，这些输出神经是用来给其他神经元传递信号或者传送信息的。</p>
<p>简而言之，神经元是一个计算单元，它从<strong>输入神经</strong>接受一定数目的信息，并做一些计算，然后将结果通过它的<strong>轴突</strong>传送到大脑中的其他神经元。</p>
<h4 id="神经网络逻辑单元"><a href="#神经网络逻辑单元" class="headerlink" title="神经网络逻辑单元"></a>神经网络逻辑单元</h4><p>在一个计算机的神经网络里，我们将使用一个非常简单的模型来模拟神经元的工作：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/001.jpg">
<p>这里的$h_\theta(x)$通常值是<br>$$<br>h_\theta(x) = \frac{1}{1+e^{(-\theta^Tx)}}<br>$$<br>其中$x$和$\theta$指的是参数向量：<br>$$<br>x = \left[<br>\begin{array}{}<br>x_0 \<br>x_1 \<br>x_2 \<br>x_3<br>\end{array}{}\right] ~~<br>\theta = \left[<br>\begin{array}{}<br>\theta_0 \<br>\theta_1 \<br>\theta_2 \<br>\theta_3<br>\end{array}{}<br>\right]<br>$$<br>这就是一个简单的模拟神经元的模型。</p>
<p>当绘制神经网络时，有时会额外增加一个$x_0$的输入节点，这个$x_0$节点有时也被称作<strong>偏置单位（或偏置神经元）</strong>。但由于$x_0=1$，所以有时不会画出。</p>
<h4 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h4><p>在神经网络中，有时会听到“一个有Sigmoid函数或者Logistic函数作为<strong>激励函数(activation function)</strong>的人工神经元”这样的话。其实这里所指的<strong>激励函数(activation function)</strong>只是对类似非线性函数$g(z)$的另一个术语称呼。</p>
<blockquote>
<p>在神经网络的文献里，模型的<strong>权重(weight)</strong>其实和模型的参数$\theta$是一个东西</p>
</blockquote>
<h4 id="解读神经网络"><a href="#解读神经网络" class="headerlink" title="解读神经网络"></a>解读神经网络</h4><h5 id="输入层，输出层，隐藏层"><a href="#输入层，输出层，隐藏层" class="headerlink" title="输入层，输出层，隐藏层"></a>输入层，输出层，隐藏层</h5><p>神经网络其实就是不同的神经元组合在一起的集合：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/002.jpg">
<p>具体来说：</p>
<ul>
<li>上图中有三个输入单元：$x_1, x_2 和 x_3$，当然也可以加入值为1的$x_0$。</li>
<li>中间有三个神经元：$a_1^{(2)}, a_2^{(2)}和a_3^{(2)}$，同理，你也可以加上值永远为1的偏执单元$a_0^{(2)}$。</li>
<li>然后在最右层有第三层，第三层的这个节点输出了假设函数$h(x)$的计算结果$h_\Theta(x)$。</li>
</ul>
<p>用神经网络的术语来说，第一层也别成为<strong>输入层</strong>，因为我们在这一层输入了特征项$x_1, x_2 和x_3$。</p>
<p>最后一层也被称为<strong>输出层</strong>，因为这一层的神经元会输出假设函数的最终计算结果$h_\Theta(x)$。</p>
<p>中间层，也被称为<strong>隐藏层</strong>，在监督学习中，你能看到输入和输出，而隐藏层的值在你的训练过程中是看不到的，它的值不是$x$也不是$y$，所以我们叫它隐藏层。神经网络可以有不止一个隐藏层。在神经网络中，任何一个非输入层且非输出层，就被称为隐藏层。</p>
<h5 id="神经网络运行原理"><a href="#神经网络运行原理" class="headerlink" title="神经网络运行原理"></a>神经网络运行原理</h5><img src="/2017/08/17/Stanford-MachineLearning-week4/003.jpg">
<p>接下来逐步分析上图所呈现的神经网络的计算步骤。</p>
<p>首先需要说明以下两个符号的含义：</p>
<ul>
<li>$a_i^{(j)}$表示第j层的第i个神经元。</li>
</ul>
<blockquote>
<p>具体来说$a_1^2$表示的是第2层的第1个激励，即隐藏层的第一个激励。</p>
<p>所谓<strong>激励(activation)</strong>是指由一个具体神经元读入计算并输出的值。</p>
</blockquote>
<ul>
<li>$\Theta^{(j)}$表示层与层之间权重的<strong>参数矩阵（权重矩阵）</strong>。</li>
</ul>
<p>具体来说，$a_1^{(2)}$的值的计算是这样的：<br>$$<br>a_1{(2)} = g(\theta_{10}^1x_0 + \theta_{11}^1x_1 + \theta_{12}^1x_2 + \theta_{13}^1x_3)<br>$$</p>
<blockquote>
<p>这里的$g$函数是S型函数（或者说是S激励函数，也叫做逻辑激励函数）。</p>
</blockquote>
<p>我们可以把隐藏层的三个神经元的计算结果都写出来：<br>$$<br>a_1{(2)} = g(\theta_{10}^1x_0 + \theta_{11}^1x_1 + \theta_{12}^1x_2 + \theta_{13}^1x_3) \<br>a_2{(2)} = g(\theta_{20}^1x_0 + \theta_{21}^1x_1 + \theta_{22}^1x_2 + \theta_{23}^1x_3) \<br>a_3{(2)} = g(\theta_{30}^1x_0 + \theta_{31}^1x_1 + \theta_{32}^1x_2 + \theta_{33}^1x_3)<br>$$<br>这里我有三个输入单元和三个隐藏单元，这样以来，参数矩阵$\Theta^{(1)}$控制了来自三个输入单元到三个隐藏单元的映射。因此$\Theta^1$的维数是$R^{3\times4}$的矩阵（考虑$x_0$的情况下）。</p>
<p>更一般的，如果一个神经网络在第$j$层有$s_j$个单元，在$j+1$层有$s_{j+1}$个单元，那么第$j$层的参数矩阵$\Theta^{(j)}$的维数就是$s_{j+1} \times (s_j + 1)$。</p>
<p>以上我们讨论了三个隐藏单位是怎么计算它们的值的。最后，在输入层，我们还有一个单元，它用来计算$h_\Theta(x)$：<br>$$<br>h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})<br>$$<br>以上就是从数学上对一个人工神经网络的定义。</p>
<h3 id="神经网络的表示Ⅱ"><a href="#神经网络的表示Ⅱ" class="headerlink" title="神经网络的表示Ⅱ"></a>神经网络的表示Ⅱ</h3><h4 id="前向传播-forward-propagation-的向量化实现"><a href="#前向传播-forward-propagation-的向量化实现" class="headerlink" title="前向传播(forward propagation)的向量化实现"></a>前向传播(forward propagation)的向量化实现</h4><p>以这个神经网络为例：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/004.jpg">
<p>如果你仔细观察表达式中红色方框内的区域：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/005.jpg">
<p>这里其实是一个矩阵的乘法运算：<br>$$<br>\Theta^{(1)}x<br>$$<br>这样一来，我们就能将神经网络的计算向量化了。</p>
<p>具体而言，我们定义特征向量$x$：<br>$$<br>x = \left[<br>\begin{array}{}<br>x_0 \<br>x_1 \<br>x_2 \<br>x_3<br>\end{array}<br>\right]<br>$$</p>
<blockquote>
<p>其中$x_0 = 1$</p>
</blockquote>
<p>并定义$z^{(2)}$：<br>$$<br>z^{(2)} = \left[<br>\begin{array}{}<br>z_1^{(2)} \<br>z_2^{(2)} \<br>z_3^{(2)}<br>\end{array}<br>\right]<br>$$</p>
<blockquote>
<p>注意，这里的$z^{(2)}$是一个三维向量。</p>
</blockquote>
<p>我们只需要两个步骤就可以计算出$a^{(2)}$向量了“<br>$$<br>z^{(2)} = \Theta^{(1)}x \<br>a^{(2)} = g(z^{(2)})<br>$$</p>
<blockquote>
<p>注意这里的$a^{(2)}$也是一个三维向量，这里的激励函数$g$将对$z^{(2)}$中的每个元素进行计算。</p>
<p>定义$a^{(1)} = x$，这样就有了向量$a^{(1)}$。</p>
</blockquote>
<p>将向量$x$替换为向量$a^{(1)}$：<br>$$<br>z^{(2)} = \Theta^{(1)}a^{(1)}<br>$$<br>然后还需要加上偏执单元$a_0^{(2)}$，此时向量$a^{(2)}$的长度变成了4：<br>$$<br>a^{(2)} \in \mathbb{R}^4<br>$$<br>最后，为了计算实际输出值：<br>$$<br>h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})<br>$$<br>我们计算出代表$\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}$的$z^{(3)}$，并将其带入激励函数，最后就能得出$h(x)$的值了：<br>$$<br>z^{(3)} = \Theta^{(2)}a^{(2)} \<br>h_\Theta(x) = a^{(3)} = g(z^{(3)})<br>$$<br>这就是计算$h_\Theta(x)$的过程，也称为<strong>前向传播(forward propagation)</strong>。</p>
<h4 id="神经网络的架构"><a href="#神经网络的架构" class="headerlink" title="神经网络的架构"></a>神经网络的架构</h4><p>你还可以用其他类型的图来表示神经网络。神经网络中神经元相连接的方式，称为<strong>神经网络的架构(Architecture)</strong>。</p>
<p>下面是另一个神经网络架构的例子：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/006.jpg">
<h2 id="2-神经网络应用实例"><a href="#2-神经网络应用实例" class="headerlink" title="(2) 神经网络应用实例"></a>(2) 神经网络应用实例</h2><h3 id="神经网络应用实例"><a href="#神经网络应用实例" class="headerlink" title="神经网络应用实例"></a>神经网络应用实例</h3><h4 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h4><p>考虑下面的问题：</p>
<p>我们有二进制的输入特征$x_1$和$x_2$，它们的取值要么是0，要么是1。这个例子中，我们有一堆用×来表示的正样本和用圆圈来表示的负样本。</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/007.jpg">
<p>我们想要做到的就是有一个非线性的决策边界来区分正负样本：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/008.jpg">
<p>下面使用这个二进制输入特征的例子来描述神经网络是怎样计算的。</p>
<p>具体来讲，我们要计算的目标函数：<br>$$<br>y = x_1 XNOR x_2<br>$$</p>
<blockquote>
<p>求同或(都为真或都为假时，结果为真，否则结果为假)。</p>
</blockquote>
<p>或者也可以写作：<br>$$<br>NOT(y = x_1 XOR x_2)<br>$$</p>
<blockquote>
<p>求异或再取反。</p>
</blockquote>
<h4 id="AND-OR-NOT的实现"><a href="#AND-OR-NOT的实现" class="headerlink" title="AND, OR, NOT的实现"></a>AND, OR, NOT的实现</h4><h5 id="AND"><a href="#AND" class="headerlink" title="AND"></a>AND</h5><p>为了解释神经网络模型如何来拟合这种训练集。我们先讲解一个稍微简单一些的神经网络，它拟合了<strong>“且运算”(AND)</strong>：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/009.jpg">
<p>假设我们有二进制输入$x_1$和$x_2$，目标函数是$y = x_1 AND x_2$，为了得到一个具有单个神经元的神经网络来计算<strong>逻辑与</strong>，我们需要画出偏执单元：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/010.jpg">
<p>接下来给这个网络分配一些权重（参数）：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/011.jpg">
<p>所以我们的假设函数是：<br>$$<br>h_\Theta(x) = g(-30 + 20x_1 + 20x_2)<br>$$<br>这里$\Theta_{10}^{(1)}$就是-30，$\Theta_{11}^{(2)}$就是20，$\Theta_{12}^{(3)}$就是20。</p>
<p>接下来介绍这个小神经元是怎样计算的。</p>
<p>回忆一下激励函数$g(z)$看起来是这样的：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/012.jpg">
<p>再来看看我们的假设在各种情况下的输出：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$h_\Theta(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-30) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10)\approx 1$</td>
</tr>
</tbody>
</table>
<hr>
<h5 id="OR"><a href="#OR" class="headerlink" title="OR"></a>OR</h5><p>下面的神经网络使用同样的原理实现了“或”的功能：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/013.jpg">
<p>假设函数为：<br>$$<br>h_\Theta(x) = g(-10 + 20x_1 + 20x_2)<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$h_\Theta(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10) \approx  1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10) \approx  1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10)\approx  1$</td>
</tr>
</tbody>
</table>
<hr>
<h5 id="NOT"><a href="#NOT" class="headerlink" title="NOT"></a>NOT</h5><p>下面的神经网络使用同样的原理实现了“非”的功能：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/014.jpg">
<p>假设函数为：<br>$$<br>h_\Theta(x) = g(10 - 20x_1)<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$h_\Theta(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
</tbody>
</table>
<h4 id="一个更复杂的例子"><a href="#一个更复杂的例子" class="headerlink" title="一个更复杂的例子"></a>一个更复杂的例子</h4><p>下面的神经网络使用同样的原理实现了$(NOTX_1)AND(NOTx_2)$的功能：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/015.jpg">
<p>假设函数为：<br>$$<br>h_\Theta(x) = g(10 - 20x_1 - 20x_2)<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$h_\Theta(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-30)\approx 0$</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="求解XNOR"><a href="#求解XNOR" class="headerlink" title="求解XNOR"></a>求解XNOR</h4><p>接下来我们使用上面求解的以下三个神经网络，就可以来运算$x_2XNORx_2$了：</p>
<img src="/2017/08/17/Stanford-MachineLearning-week4/016.jpg">

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/07/Octave-LinearRegression/" rel="next" title="Octave实现线性回归模型">
                <i class="fa fa-chevron-left"></i> Octave实现线性回归模型
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/29/Stanford-MachineLearning-week5/" rel="prev" title="斯坦福机器学习课程 第五周">
                斯坦福机器学习课程 第五周 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/head.jpg"
               alt="El Chiang" />
          <p class="site-author-name" itemprop="name">El Chiang</p>
           
              <p class="site-description motion-element" itemprop="description">超级好奇</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/El-Chiang" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/5096104875?refer_flag=1005055010_&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-神经网络"><span class="nav-number">1.</span> <span class="nav-text">(1) 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的表示Ⅰ"><span class="nav-number">1.1.</span> <span class="nav-text">神经网络的表示Ⅰ</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络起源"><span class="nav-number">1.1.1.</span> <span class="nav-text">神经网络起源</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络逻辑单元"><span class="nav-number">1.1.2.</span> <span class="nav-text">神经网络逻辑单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#激励函数"><span class="nav-number">1.1.3.</span> <span class="nav-text">激励函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解读神经网络"><span class="nav-number">1.1.4.</span> <span class="nav-text">解读神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#输入层，输出层，隐藏层"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">输入层，输出层，隐藏层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#神经网络运行原理"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">神经网络运行原理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的表示Ⅱ"><span class="nav-number">1.2.</span> <span class="nav-text">神经网络的表示Ⅱ</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向传播-forward-propagation-的向量化实现"><span class="nav-number">1.2.1.</span> <span class="nav-text">前向传播(forward propagation)的向量化实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络的架构"><span class="nav-number">1.2.2.</span> <span class="nav-text">神经网络的架构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-神经网络应用实例"><span class="nav-number">2.</span> <span class="nav-text">(2) 神经网络应用实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络应用实例"><span class="nav-number">2.1.</span> <span class="nav-text">神经网络应用实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题引入"><span class="nav-number">2.1.1.</span> <span class="nav-text">问题引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AND-OR-NOT的实现"><span class="nav-number">2.1.2.</span> <span class="nav-text">AND, OR, NOT的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#AND"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">AND</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#OR"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">OR</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#NOT"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">NOT</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一个更复杂的例子"><span class="nav-number">2.1.3.</span> <span class="nav-text">一个更复杂的例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#求解XNOR"><span class="nav-number">2.1.4.</span> <span class="nav-text">求解XNOR</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-paw"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">El Chiang</span>
</div>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
